# ì„¤ì¹˜ í•„ìš”:
# pip install streamlit transformers sentencepiece faiss-cpu python-docx PyMuPDF scikit-learn keybert

import os
import fitz
import docx
import numpy as np
import faiss
import pickle
import streamlit as st
from sklearn.metrics.pairwise import cosine_similarity
from keybert import KeyBERT
import torch
from transformers import AutoTokenizer, AutoModel

# --- 1ï¸âƒ£ ë¬¸ì„œ ì½ê¸° í•¨ìˆ˜ ---
def read_document_paragraphs(file_path_or_file):
    paragraphs = []
    if hasattr(file_path_or_file, "read"):
        # UploadedFile ê°ì²´ ì²˜ë¦¬
        if file_path_or_file.name.endswith(".pdf"):
            doc = fitz.open(stream=file_path_or_file.read(), filetype="pdf")
            for page in doc:
                text = page.get_text("text")
                for p in text.split("\n"):
                    if p.strip():
                        paragraphs.append(p.strip())
        elif file_path_or_file.name.endswith(".docx"):
            docx_file = docx.Document(file_path_or_file)
            for p in docx_file.paragraphs:
                if p.text.strip():
                    paragraphs.append(p.text.strip())
    else:
        # ë¡œì»¬ ê²½ë¡œ ì²˜ë¦¬
        if file_path_or_file.endswith(".pdf"):
            doc = fitz.open(file_path_or_file)
            for page in doc:
                text = page.get_text("text")
                for p in text.split("\n"):
                    if p.strip():
                        paragraphs.append(p.strip())
        elif file_path_or_file.endswith(".docx"):
            if os.path.basename(file_path_or_file).startswith("~$"):
                return []
            d = docx.Document(file_path_or_file)
            for p in d.paragraphs:
                if p.text.strip():
                    paragraphs.append(p.text.strip())
    return paragraphs

# --- 2ï¸âƒ£ Streamlit ì„¤ì • ---
st.set_page_config(page_title="Document Search & Keyword System", layout="wide")
st.title("ë¬¸ì„œ ê²€ìƒ‰ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ. TEAM TechTree")
st.info("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ documents í´ë”ì— ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ ë²¡í„°í™”ë©ë‹ˆë‹¤.")

status_message = st.empty()
status_message.info("ëª¨ë¸ ë¡œë“œ ì¤‘... (ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")

# --- 3ï¸âƒ£ Hugging Face LaBSE ëª¨ë¸ CPU ë¡œë“œ ---
model_name = "sentence-transformers/LaBSE"
tokenizer = AutoTokenizer.from_pretrained(model_name)
hf_model = AutoModel.from_pretrained(model_name)
hf_model.eval()
hf_model.to("cpu")

# ë¬¸ì¥ ì„ë² ë”© ìƒì„± í•¨ìˆ˜
@torch.no_grad()
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = hf_model(**inputs)
    # CLS í† í° ë²¡í„° í‰ê· 
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.squeeze().numpy()

# KeyBERT ëª¨ë¸
kw_model = KeyBERT(model=None)  # transformers ëª¨ë¸ ì§ì ‘ ì‚¬ìš© ì˜ˆì •

stopwords_ko = ["ì€", "ëŠ”", "ì´", "ê°€", "ì˜", "ì—", "ì„", "ë¥¼", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ"]

status_message.success("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# --- 4ï¸âƒ£ FAISS DB ê²½ë¡œ ---
index_file = "vector_index.faiss"
data_file = "doc_data.pkl"

# --- 5ï¸âƒ£ DB ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„± ---
status_message.info("ë²¡í„° DB ì¤€ë¹„ ì¤‘...")
embedding_dim = hf_model.config.hidden_size
if os.path.exists(index_file) and os.path.exists(data_file):
    index = faiss.read_index(index_file)
    with open(data_file, "rb") as f:
        data = pickle.load(f)
        doc_names = data["names"]
        doc_paragraphs = data["paragraphs"]
        doc_embeddings = data["embeddings"]
        doc_keywords = data["keywords"]
    status_message.success("ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì™„ë£Œ!")
else:
    index = faiss.IndexFlatL2(embedding_dim)
    doc_names, doc_paragraphs, doc_embeddings, doc_keywords = [], [], [], []
    status_message.success("ìƒˆ ë²¡í„° DB ìƒì„± ì™„ë£Œ!")

# --- 6ï¸âƒ£ ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ ---
def process_file(file_path_or_file, file_name, progress_bar=None, progress_offset=0, total_paragraphs=1):
    paragraphs = read_document_paragraphs(file_path_or_file)
    for i, p in enumerate(paragraphs):
        if (file_name, i) in [(d[0], d[1]) for d in doc_paragraphs]:
            continue

        emb = get_embedding(p)
        index.add(np.array([emb], dtype="float32"))
        doc_names.append(file_name)
        doc_paragraphs.append((file_name, i))
        doc_embeddings.append(emb)

        keywords = kw_model.extract_keywords(
            p, keyphrase_ngram_range=(1, 2), stop_words="english", top_n=10
        )
        keywords_list = [kw for kw, score in keywords if not any(sw in kw for sw in stopwords_ko)]
        doc_keywords.append(keywords_list)

        if progress_bar is not None:
            value = min((progress_offset + i + 1) / total_paragraphs, 1.0)
            progress_bar.progress(value)

# --- 7ï¸âƒ£ documents í´ë” ì²˜ë¦¬ ---
if not os.path.exists("./documents"):
    os.makedirs("./documents")

doc_files = [f for f in os.listdir("./documents") if f.endswith(".pdf") or f.endswith(".docx")]
if doc_files:
    status_message.info("documents í´ë” ë¬¸ì„œ ë²¡í„°í™” ì¤‘...")
    progress_bar = st.progress(0)
    total_paragraphs = sum(len(read_document_paragraphs(os.path.join("./documents", f))) for f in doc_files)
    paragraph_offset = 0
    for file_name in doc_files:
        file_path = os.path.join("./documents", file_name)
        process_file(file_path, file_name, progress_bar=progress_bar, progress_offset=paragraph_offset, total_paragraphs=total_paragraphs)
        paragraph_offset += len(read_document_paragraphs(file_path))
    progress_bar.empty()
    status_message.success("documents í´ë” ë¬¸ì„œ ë²¡í„°í™” ì™„ë£Œ!")

# --- 8ï¸âƒ£ Streamlit ì—…ë¡œë“œ ì²˜ë¦¬ ---
with st.expander("ë¬¸ì„œ ì—…ë¡œë“œ ë° ë²¡í„°í™”", expanded=True):
    uploaded_files = st.file_uploader("ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš” (.pdf ë˜ëŠ” .docx)", accept_multiple_files=True)
    if uploaded_files:
        progress_bar = st.progress(0)
        total_paragraphs = 0
        temp_paths = []

        for file in uploaded_files:
            file_name = file.name
            if file_name.startswith("~$") or not (file_name.endswith(".pdf") or file_name.endswith(".docx")):
                continue
            temp_path = os.path.join("./documents", file_name)
            with open(temp_path, "wb") as f:
                f.write(file.getbuffer())
            temp_paths.append((file_name, temp_path))
            total_paragraphs += len(read_document_paragraphs(temp_path))

        paragraph_offset = 0
        for file_name, temp_path in temp_paths:
            process_file(
                file_path_or_file=temp_path,
                file_name=file_name,
                progress_bar=progress_bar,
                progress_offset=paragraph_offset,
                total_paragraphs=total_paragraphs
            )
            paragraph_offset += len(read_document_paragraphs(temp_path))

        progress_bar.empty()
        st.success("ì—…ë¡œë“œëœ ë¬¸ì„œ ë²¡í„°í™” ë° í‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ!")

# --- 9ï¸âƒ£ DB ì €ì¥ ---
faiss.write_index(index, index_file)
with open(data_file, "wb") as f:
    pickle.dump({
        "names": doc_names,
        "paragraphs": doc_paragraphs,
        "embeddings": doc_embeddings,
        "keywords": doc_keywords
    }, f)

# --- ğŸ”Ÿ ê²€ìƒ‰ ê¸°ëŠ¥ ---
with st.expander("ë¬¸ì„œ ê²€ìƒ‰", expanded=True):
    query = st.text_input("ê²€ìƒ‰ì–´ ì…ë ¥")
    top_k = st.slider("ìƒìœ„ ëª‡ ê°œ ê²°ê³¼ë¥¼ ë³´ì—¬ë“œë¦´ê¹Œìš”?", 1, 10, 5)
    if st.button("ê²€ìƒ‰") and query:
        if len(doc_embeddings) == 0:
            st.warning("ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            query_emb = get_embedding(query).reshape(1, -1)
            k = min(top_k, len(doc_embeddings))
            distances, indices = index.search(np.array(query_emb, dtype="float32"), k)

            st.subheader(f"ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {k}ê°œ)")
            for rank, idx in enumerate(indices[0]):
                sim = cosine_similarity([query_emb[0]], [doc_embeddings[idx]])[0][0]
                doc_file, para_idx = doc_paragraphs[idx]
                st.markdown("---")
                st.markdown(f"**{rank+1}. {doc_file} - ë¬¸ë‹¨ {para_idx}**")
                st.markdown(f"- ìœ ì‚¬ë„: {sim:.4f}, ê±°ë¦¬: {distances[0][rank]:.4f}")
                st.markdown(f"- í‚¤ì›Œë“œ: {', '.join(doc_keywords[idx])}")
